{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/kdysunleo/NMT/main/eng-kor.txt"
      ],
      "metadata": {
        "id": "6EWqXlQSt_bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837f0beb-50e9-4ef1-cbb8-c9b4d47784d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-06 05:05:37--  https://raw.githubusercontent.com/kdysunleo/NMT/main/eng-kor.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534332 (522K) [text/plain]\n",
            "Saving to: ‘eng-kor.txt.1’\n",
            "\n",
            "\reng-kor.txt.1         0%[                    ]       0  --.-KB/s               \reng-kor.txt.1       100%[===================>] 521.81K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-06 05:05:37 (36.0 MB/s) - ‘eng-kor.txt.1’ saved [534332/534332]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbkYvQ47ayQ"
      },
      "source": [
        "**Sequence to Sequence 네트워크와 Attention을 이용한 Machine Translation 모델 구현**\n",
        "\n",
        "- 한국어를 영어로 번역하도록 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2cxUfvGzpcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd71b1ec-5e7e-417c-9dd8-e15337657aec"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed = 21\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc1fc225530>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQ4C8wO5QcV"
      },
      "source": [
        "언어별 정보 저장을 위한 클래스 생성\n",
        "\n",
        "- 단어->아이디, 아이디->단어 사전 생성 (word2idx)\n",
        "- addSentence 함수는 문장을 띄어쓰기를 기준으로 토큰화\n",
        "- addWord 함수\n",
        "    - word2idx, idx2word 만들기\n",
        "    - 단어별 출현 횟수 구하기\n",
        "    - 전체 단어의 개수 구하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5V2jJCG0Qot"
      },
      "source": [
        "\"\"\"\n",
        "언어별 정보 저장을 위한 클래스 생성\n",
        "\n",
        "- 단어->아이디, 아이디->단어 사전 생성 (word2idx)\n",
        "- addSentence 함수는 문장을 띄어쓰기를 기준으로 토큰화\n",
        "- addWord 함수\n",
        "    - word2idx, idx2word 만들기\n",
        "    - 단어별 출현 횟수 구하기\n",
        "    - 전체 단어의 개수 구하기\n",
        "\"\"\"\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        ## 단어->아이디, 단어별 개수 사전 선언\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"} # word2index, word2count 에는 안넣어도 됨\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        # sentence를 띄어쓰기 단위로 토큰화 하고, 토큰은 addWord 함수를 이용해서 정보 저장\n",
        "\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        # word2idx, word2count, idx2word, nwords 정보 저장\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        if word in self.word2index:\n",
        "          self.word2count[word]+= 1\n",
        "        else:\n",
        "          self.word2index.update({ word: self.n_words})\n",
        "          self.word2count[word] = 1\n",
        "          self.index2word.update({self.n_words: word})\n",
        "          self.n_words += 1\n",
        "\n",
        "        ###############################################"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp3YN2_M7FSy"
      },
      "source": [
        "- 병렬 코퍼스 준비 : 영어-한국어\n",
        "- 영어는 normalize를 진행\n",
        "- readLangs 함수\n",
        "    - 파일을 읽고 쌍으로 분리\n",
        "    - Lang class를 이용하여 각 언어 정보 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1N6GRxS5d5-"
      },
      "source": [
        "\"\"\"\n",
        "- 병렬 코퍼스 준비 : 영어-한국어\n",
        "- 영어는 normalize를 진행\n",
        "- readLangs 함수\n",
        "    - 파일을 읽고 쌍으로 분리\n",
        "    - Lang class를 이용하여 각 언어 정보 저장\n",
        "\"\"\"\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # ?!앞에 띄어쓰기 추가 (별도 토큰 처리를 위해서)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # git clone을 통해 데이터를 다운로드 받는 경우, 파일 경로 수정 필요\n",
        "    f = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8')\n",
        "    lines = f.readlines()\n",
        "\n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "        line = l.split(\"\\t\")\n",
        "        # 한국어, 영어\n",
        "        pair = [line[1], normalizeString(line[0])]\n",
        "        pairs.append(pair)\n",
        "\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "    f.close()\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voejo56ravk2"
      },
      "source": [
        "- MAX_LENGTH : 학습에 사용할 최대문장길이\n",
        "- eng_prefixes : 해당 단어들로 시작하는 문장만 번역에 포함\n",
        "\n",
        "- filterPair : 위 조건에 만족하는 pair만 반환\n",
        "- filterPairs : 필터링된 pair만 저장해서 반환"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ( 1 > 0 and 2 > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8mEt8rkqcz_",
        "outputId": "67fd5923-5062-4f8e-e302-eb05b2dc99a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jqvzt5j5on-"
      },
      "source": [
        "MAX_LENGTH = 30\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 영어는 해당하는 구문으로 시작하고 영어와 한국어 둘다 max_length보다 작은 데이터일 경우에만 True 반환\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    # filterPair가 True인 데이터만 저장후 반환\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00R1vJMkbeg5"
      },
      "source": [
        "데이터 준비 및 결과 확인\n",
        "- **학습과 검증에 필요한 데이터쌍을 구분**\n",
        "- 이전 단계에서 만든 클래스와 함수를 통해 결과 확인\n",
        "- 전체 문장 개수(pair 개수)\n",
        "- 필터링 후 문장개수(pair 개수)\n",
        "- 각 언어별로, 사전에 등록된 단어 개수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b318yrC5vEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b9d72f-480c-42ff-f21c-fafe645b32d6"
      },
      "source": [
        "def prepareData(lang1, lang2):\n",
        "\n",
        "    # 이전에 만든 클래스를 활용해서 영어 한국어의 정보를 저장\n",
        "    # 조건에 맞게 데이터쌍을 필터링\n",
        "    # 필터링된 데이터쌍의 80%는 학습용으로, 나머지 20%는 검증용으로 구분\n",
        "    # 전체 데이터쌍의 수, 필터링 후 데이터쌍의 수 그리고 각 언어별 단어(토큰)의 개수를 출력\n",
        "\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "    # 데이터 shuffle\n",
        "    # 데이터를 8:2로 split\n",
        "    # 각 언어 객체에 문장 추가\n",
        "    ###############################################\n",
        "\n",
        "    random.shuffle(pairs)\n",
        "    train_size = int(len(pairs) * 0.8)\n",
        "    print(\"# of 80 percent of data: \", train_size)\n",
        "    train_pairs = pairs[:train_size]\n",
        "    valid_pairs = pairs[train_size:]\n",
        "\n",
        "    for pair in train_pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(\"input_lang:\", input_lang.name, input_lang.n_words)\n",
        "    print(\"output_lang:\", output_lang.name, output_lang.n_words)\n",
        "\n",
        "    return input_lang, output_lang, train_pairs, valid_pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, train_pairs, valid_pairs = prepareData('eng', 'kor')\n",
        "print(random.choice(train_pairs))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 3648 sentence pairs\n",
            "Trimmed to 223 sentence pairs\n",
            "# of 80 percent of data:  178\n",
            "input_lang: kor 445\n",
            "output_lang: eng 304\n",
            "['그걸 들어야 했다니 유감이야.', 'i m sorry that you had to hear that .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNGZrKSHCKKE"
      },
      "source": [
        "인코더 역할의 RNN 생성\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi9HMKT_LPN2"
      },
      "source": [
        "\"\"\"\n",
        "인코더 역할의 RNN 생성\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기\n",
        "\"\"\"\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        embedding_size = hidden_size\n",
        "        # vocab size x hidden_dim word2vector나 glove embedding으로 초기화 할수도 있음\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        ####################  빈칸  ####################\n",
        "        # 임베딩 만들고, gru에 입력으로 넣어주고 output 과 hidden 얻기\n",
        "        ###############################################\n",
        "        # 0, 1차원은 1, 2차원이 vocab size, -1이 의미하는건 나머지 차원을 고려했을때 자동값을 넣어라 (hidden_size) => [[[0.1, 0.2]]]\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # word idx로부터 word embedding만들기\n",
        "        output, hidden = self.gru(embedded, hidden) # gru # (batch _size x seq_len x hidden_emb) or (seq_len x batch_size x hidden_emb)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vedfHYCXEjqL"
      },
      "source": [
        "디코더 역할의 RNN 생성\n",
        "\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기\n",
        "- softmax를 이용하여 전체 단어에서 가장 확률높은 단어 뽑기\n",
        "\n",
        "```\n",
        "# 코[드로 형식 지정됨\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨](https:// [링크 텍스트](https://))\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhPhntc_LQUG"
      },
      "source": [
        "\"\"\"\n",
        "디코더 역할의 RNN 생성\n",
        "\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기\n",
        "- softmax를 이용하여 전체 단어에서 가장 확률높은 단어 뽑기\n",
        "\"\"\"\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # GRU의 결과가 linear layer(classifier) 와 softmax를 거쳐야 한다\n",
        "        # 차원 정의\n",
        "        # layer 정의\n",
        "        # log prob 계산\n",
        "        ###############################################\n",
        "        self.hidden_size = hidden_size\n",
        "        embedding_size = hidden_size\n",
        "        # layer 정의\n",
        "        # vocab size x hidden_dim word2vector나 glove embedding으로 초기화 할수도 있음\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        # log prob 계산\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # 임베딩 만들고, gru 입력으로 넣어주고,\n",
        "        # output을 linear 와 softmax의 입력으로 넣어주기\n",
        "\n",
        "        # batch_size = 1\n",
        "        # input (단어 index): batch_size x 1 (seq_length)\n",
        "        # self.embedding(input): batch_size x embeddging_size (embedding_size = hidden_size)\n",
        "        # output = self.embedding(input).viuew(1,1,01)\n",
        "        # hidden (이전 step의 hidden vector): batch_size x hidden_size\n",
        "        # output.hidden = self.gru(ouput, hidden) :\n",
        "        # word embedding (1x1xembedding_size)\n",
        "\n",
        "        output = self.embedding(input).view(1,1,-1)\n",
        "        output, hidden = self.gru(output, hidden) # gru, encoder의 hidden 을 init\n",
        "\n",
        "        # attention 연산이 들어갈 부분\n",
        "        # batch_size(1) x seq_length(1) x embedding_dim\n",
        "        # seq_length x embedding\n",
        "        # output[:, 0] => output[0]\n",
        "        ouput = self.log_softmax(self.out(output[:, 0])) # decoder  gru의 output으로 log prob을 계산\n",
        "\n",
        "\n",
        "        ###############################################\n",
        "        return output, hidden"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhJleo9-epbA"
      },
      "source": [
        "학습 데이터 준비\n",
        "- 각 문장마다 word2idx를 이용하여 tensor로 변환(벡터화)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zqohs_iHEK3"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\"\"\"\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\"\"\"\n",
        "\n",
        "def indexesFromSentence(lang, sentence) -> List[int]:\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 문장을 띄어쓰기 단위로 토큰화 하고, 해당 단어(토큰)의 id로 변환\n",
        "    # 단, 검증때 사전에 없는 단어가 출현할 수 있는 상황도 처리할 수 있어야 함\n",
        "    indexes = []\n",
        "\n",
        "    for word in sentence.split(\" \"):\n",
        "      if lang.word2index[word]:\n",
        "        indexes.append(lang.word2index[word])\n",
        "      else:\n",
        "        indexes.append(UNK_token)\n",
        "    ###############################################\n",
        "    return indexes\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 실제 데이터 외에 추가로 들어가야할 토큰이 있다!\n",
        "    indexes.append(EOS_token)\n",
        "    ###############################################\n",
        "\n",
        "    # seq_length x 1\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9VC8TpcfAFN"
      },
      "source": [
        "모델 학습 코드\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGo9aTzMNVPY"
      },
      "source": [
        "teacher_forcing_ratio = 0.5 #\n",
        "\n",
        "# train for each step\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    # encoder 초기값 설정, gradient 0 으로 초기화\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "    # encoder 호출\n",
        "    for ei in range(input_length):\n",
        "        # 각 token에 대해서 hidden과 output을 계산\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "\n",
        "    # decoder의 초기 input과 hidden값 결정\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device) # SOS 토큰\n",
        "    decoder_hidden = encoder_hidden # encoder의 hidden을 decoder의 초기 hidden으로 사용\n",
        "    ###############################################\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        ####################  빈칸  ####################\n",
        "        # 디코더의 다음 입력으로 실제값 사용\n",
        "        for di in range(target_length):\n",
        "            # decoder로부터 hidden과 output을 계산\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di]) # loss 계산\n",
        "            # 다음 decoder의 input은 정답으로부터 가져옴\n",
        "            decoder_input = target_tensor[di]\n",
        "        ###############################################\n",
        "    else: # y = ax + b, (a=2, x=2, b=1) => y = 5, y = 2 * 2 + 1\n",
        "        ####################  빈칸  ####################\n",
        "        # 디코더의 다음 입력으로 예측값 사용\n",
        "      for di in range(target_length):\n",
        "            # decoder로부터 hidden과 output을 계산\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            # decoder의 output으로부터 가장 확률이 높은 token을 다음 decoder input으로 사용\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.detach() # pytorch 기울기 계산할때 back propagation을 위해서  computational graph를 그리는데, 그 연결을 끊어줌\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di]) # loss 계산\n",
        "            # decoder가 EOS token을 생성하면 종료\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "        ###############################################\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzACV8YvgWbQ"
      },
      "source": [
        "시간측정 출력 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNTER1wWNZqg"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n50TJ_uKgawe"
      },
      "source": [
        "학습 진행 과정\n",
        "- 타이머 시작\n",
        "- optimizer, criterion 선언\n",
        "- 전체 학습 데이터에 iteration 수만큼 랜덤하게 데이터 구축"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myAFYp1iNcj7"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "    # optimizer 선언, 데이터 준비, loss함수 선언\n",
        "    # optimizer 선언 - optimizer는 바꿔도 되는데, optimizer마다 최적 lr 값이 다르므로 적절히 설정하는게 중요함 ex) SGD: 1e-2, Adam 1e-4\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) # Adam / AdamW\n",
        "\n",
        "    # 학습 데이터에서 random하게 선택\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs)) for i in range(n_iters)]\n",
        "    # Negative Log Likelihood Loss 정의\n",
        "    # softmax+ cross entropy loss(CE loss) => log_softmax + negative log likelihood (NLL loss)\n",
        "    criterion = nn.NLLLoss()\n",
        "    ###############################################\n",
        "\n",
        "    # n_iter 횟수만큼 모델 학습 및 로스 출력\n",
        "    for iter in range(1, n_iters + 1):\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # 학습에 사용할 데이터 input, target 가져오기\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        # 학습 및 loss 반환\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        ###############################################\n",
        "\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCAYkwZANkZO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "f2b95289-f795-42a3-8aff-a825cf8c8ef4"
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "####################  빈칸  ####################\n",
        "# encoder, decoder를 선언하고, 7500번의 iteration으로 학습하고 1000번마다 loss 출력\n",
        "# 모델 정의\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# 학습\n",
        "trainIters(encoder1, decoder1, 7500, print_every=1000)\n",
        "###############################################\n",
        ""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-efbddb8de914>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-ee399596e19b>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, learning_rate)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 학습 및 loss 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0m\u001b[1;32m     28\u001b[0m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-29861676ac51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pytorch 기울기 계산할때 back propagation을 위해서  computational graph를 그리는데, 그 연결을 끊어줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;31m# decoder가 EOS token을 생성하면 종료\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEOS_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2702\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2704\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [1, 256], got [1]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDwhujmDi3f_"
      },
      "source": [
        "모델 검증 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfh6b3c8Nhpj"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad(): # gd 수행을 안할꺼다, 메모리 절약, 속도 최적화\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # input 데이터에서 문장 가져와서 텐서로 변환 (train과 동일)\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        # 인코더의 초기 hidden state를 결정 (train과 동일)\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # decoder의 초기 input 과 hidden state 결정 (train과 동일)\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            ####################  빈칸  ####################\n",
        "            # decoding 멈출 시기 결정, 예측단어 decoded_words에 저장\n",
        "\n",
        "            # EOS_token을 생성하면 종료\n",
        "            # topi = tensor([3])\n",
        "            # topi.item() = 3\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('')\n",
        "                break\n",
        "            else:\n",
        "                # 예측한 단어를 추가\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            ###############################################\n",
        "\n",
        "\n",
        "            decoder_input = topi\n",
        "        return decoded_words"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBzakG5_jix7"
      },
      "source": [
        "검증 데이터쌍에서 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDOlyL1ANiph"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "eval_pair = [['너 건망증이 좀 있구나.',\"You're quite forgetful.\"],\n",
        "             ['미안한데, 내가 도와줄 수가 없어.',\"I'm sorry, I can't help you.\"],\n",
        "             ['너 떨고 있네.',\"You're shivering.\"],\n",
        "             ['그는 우울하다.',\"He is depressed.\"],\n",
        "             ['난 네 선생이다.',\"I'm your teacher.\"],\n",
        "             ['내 피가 끓고 있었다.\t',\"My blood was boiling.\"]]\n",
        "\n",
        "def evaluatePrint(encoder, decoder):\n",
        "    for pair in eval_pair:\n",
        "        print('원본: ', pair[0])\n",
        "        print('정답 변역: ', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        print('예측 번역: ', output_sentence)\n",
        "        print(\"sentence_bleu: %.3f\" % (sentence_bleu([normalizeString(pair[1]).split()], output_sentence.split(), smoothing_function=smoothie)))\n",
        "        print('')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chh42lMFs322"
      },
      "source": [
        "전체 검증 데이터의 BLEU score 평균 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0m-U4b3mgG0"
      },
      "source": [
        "def evaluateScore(encoder, decoder):\n",
        "    score = 0.0\n",
        "    for pair in valid_pairs:\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        score += sentence_bleu([normalizeString(pair[1]).split()], output_sentence.split(), smoothing_function=smoothie)\n",
        "    print('Score : ',score/len(valid_pairs))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uxAFLElQiVa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "67395ad5-81b6-402e-8bda-175a0a39a55b"
      },
      "source": [
        "evaluatePrint(encoder1, decoder1)\n",
        "evaluateScore(encoder1, decoder1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본:  너 건망증이 좀 있구나.\n",
            "정답 변역:  You're quite forgetful.\n",
            "예측 번역:  an trainee happiest instincts beautiful see concentrating i good die die learning his had charity depressed about in hard intrigued quite religious calling having requirements concentrating i good die\n",
            "sentence_bleu: 0.010\n",
            "\n",
            "원본:  미안한데, 내가 도와줄 수가 없어.\n",
            "정답 변역:  I'm sorry, I can't help you.\n",
            "예측 번역:  an trainee happiest instincts beautiful see concentrating i good die die learning his had charity depressed about in hard intrigued quite religious calling having requirements concentrating i good die\n",
            "sentence_bleu: 0.011\n",
            "\n",
            "원본:  너 떨고 있네.\n",
            "정답 변역:  You're shivering.\n",
            "예측 번역:  an trainee happiest instincts beautiful see concentrating i good die die learning his had charity depressed about in hard intrigued quite religious calling having requirements concentrating i good die\n",
            "sentence_bleu: 0.000\n",
            "\n",
            "원본:  그는 우울하다.\n",
            "정답 변역:  He is depressed.\n",
            "예측 번역:  an trainee happiest instincts beautiful see concentrating i good die die learning his had charity depressed about in hard intrigued quite religious calling having requirements concentrating i good die\n",
            "sentence_bleu: 0.010\n",
            "\n",
            "원본:  난 네 선생이다.\n",
            "정답 변역:  I'm your teacher.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-e6081a6c0cb7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluatePrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevaluateScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-466d1f17ec34>\u001b[0m in \u001b[0;36mevaluatePrint\u001b[0;34m(encoder, decoder)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'원본: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'정답 변역: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'예측 번역: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-3274a751fb74>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# input 데이터에서 문장 가져와서 텐서로 변환 (train과 동일)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-c792b90606f8>\u001b[0m in \u001b[0;36mtensorFromSentence\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m####################  빈칸  ####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-c792b90606f8>\u001b[0m in \u001b[0;36mindexesFromSentence\u001b[0;34m(lang, sentence)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '네'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9gep5hdjrGt"
      },
      "source": [
        "Attention 기법을 적용한 디코더 RNN\n",
        "- dot production 으로 attention score 구하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGComqlqNPB4"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens, input_length):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # gru를 돌고 나온 hidden state로 어텐션 구하기\n",
        "        # attn_score (dot) -> attn distribution -> attn value -> concat (attn value;hidden state)\n",
        "        # 이때, 실제 input 길이를 제외한 나머지 부분은 매우 작은 값(-9e10)으로 마스킹해준 후 softmax를 돌아야함\n",
        "        # concat 된 벡터를 linear 하나 돌아서 output 도출\n",
        "\n",
        "        # output, hidden 계산\n",
        "\n",
        "        # attn score 구하기 (내적)\n",
        "\n",
        "        # encoding_hiddens: batch_size x max_seq_length x hidden_dim\n",
        "        # input을 제외한 부분은 masking\n",
        "\n",
        "        # softmax를 통해 encoder의 hidden embedding들을 weight sum\n",
        "\n",
        "        # hidden과 encoder hidden에 어텐션을 적용한 결과를 concat하여 최종 단어 예측\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1sZGd9Enm9h"
      },
      "source": [
        "어텐션 모델 학습 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9eB47UHH7Sq"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# train for each step\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 기본 decoder를 사용했을 때와 거의 유사하다.\n",
        "    # decoder 인자에 주의하며, 위에서 작성한 코드를 참고해서 작성\n",
        "    # encoder 의 hiddens state 값들을 모아두어야 함, 미리 transpose 시키기\n",
        "\n",
        "        # 기존의 decoder 학습과 달라진 점: 모든 encdoer의 hidden을 저장 (decoder의 attn에 사용)\n",
        "\n",
        "    # decoder의 input에 encoder의 모든 hidden embedding을 사용\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOjUT78anqXZ"
      },
      "source": [
        "어텐션 모델 검증 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-yPNX7tIZf9"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 기본 decoder를 사용했을 때와 거의 유사하다.\n",
        "    # decoder 인자에 주의하며, 위에서 작성한 코드를 참고해서 작성\n",
        "    # encoder 의 hiddens state 값들을 모아두어야 함, 미리 transpose 시키기\n",
        "\n",
        "    # 기존의 decoder 학습과 달라진 점: 모든 encdoer의 hidden을 저장 (decoder의 attn에 사용)\n",
        "\n",
        "        # encoder 호출\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2dN3AjJJhZ"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 7500, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTuJZKtSJE3J"
      },
      "source": [
        "evaluatePrint(encoder1, attn_decoder1)\n",
        "evaluateScore(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ioO7txpOnXmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}